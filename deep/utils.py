#!/usr/bin/env python
# encoding: utf-8
"""
@author: liukang
@file: utils.py
@time: 2019/11/17 下午6:59
@desc:
"""

from datetime import datetime
import datetime
import re
import subprocess

from itertools import chain
from pyspark.sql import functions as F
from pyspark.sql.types import ArrayType, FloatType
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark import SparkContext

array_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())


def vector_to_array_udf(atype):
    return F.udf(lambda l: l.toArray().tolist(), ArrayType(atype))


def typed_udf(return_type):
    '''
    decorator help wrap udf
    '''

    def _typed_udf_wrapper(func):
        return F.udf(func, return_type)

    return _typed_udf_wrapper


def concat_array_cols(type):
    '''
    type is for element in array
    concat a list of array as one array
    return a udf which return a list
    '''

    def concat_(*args):
        return list(chain(*args))

    return F.udf(concat_, ArrayType(type))


def concat_cols(type):
    '''
    type is for element in array
    concat a list of array as one array
    return a udf which return a list
    '''

    def concat_(*args):
        return list(args)

    return F.udf(concat_, ArrayType(type))


def array_onehot(array):
    @typed_udf(ArrayType(FloatType()))
    def func(col):
        length = len(array)
        result = [0.0] * length
        for i in range(0, length):
            if col == array[i]:
                result[i] = 1.0
        return result

    return func


def index_onehot(dim, begin_index=0):
    @typed_udf(ArrayType(FloatType()))
    def func(col):
        result = [0.0] * dim
        result[int(col) - begin_index] = 1.0
        return result

    return func


def get_agg_dict(features, agg_type):
    '''
    help gen agg dict for list of cols
    '''
    agg_dict = {}
    for fn in features:
        agg_dict[fn] = agg_type
    return agg_dict


def get_avg_features(feature_df, features, filename, is_train):
    '''
    is_train is True:
       calculate avg of each features using training data and store it on local disk
    is_train is False:
       load avg data on disk which generated by train sample
    '''
    import pickle
    if not is_train:
        result = pickle.load(open(filename, 'r'))
        return result

    for fn in features:
        feature_df = feature_df.withColumn(fn + '_new',
                                           F.when(F.col(fn) == -999.0, 0.0).otherwise(F.col(fn)))

    new_features = [x + '_new' for x in features]
    avg_features = feature_df.agg(get_agg_dict(new_features, 'avg')).collect()
    print(avg_features)
    print('avg features:', avg_features[0])
    pickle.dump(avg_features[0], open(filename, 'w'))
    return avg_features[0]


def udf_gen_features_v2():
    def replace_nine(cols):
        result = []
        for i in range(len(cols)):
            if cols[i] == -999.0:
                result.append(0.0)
            else:
                result.append(float(cols[i]))
        return result

    return F.udf(lambda x: replace_nine(x), ArrayType(FloatType()))


def udf_gen_features(avg_features):
    '''
    wrap gen_features as spark udf
    '''
    return F.udf(lambda x: gen_features(avg_features, x), ArrayType(FloatType()))


def gen_features(avg_features, cols):
    '''
    replace -999 by avg of this feature and return a list
    '''
    result = []
    for i in range(len(cols)):
        if cols[i] == -999.0:
            result.append(float(avg_features[i]))
        else:
            result.append(float(cols[i]))
    return result


def count_tfrecords(spark, path):
    df = spark.read.format("tfrecords").option("recordType", "Example").load(path)
    return df.count()


def count_libsvm(spark, path, feature_num):
    df = spark.read.format("libsvm").option('numFeatures', str(feature_num)).load(path)
    return df.count()


def _get_hdp_path(sc: SparkContext, file_path):
    """
    创建hadoop path对象
    :param sc sparkContext对象
    :param file_path 文件绝对路径
    :return org.apache.hadoop.fs.Path对象
    """
    path_class = sc._gateway.jvm.org.apache.hadoop.fs.Path
    return path_class(file_path)


def _get_hdp_file_system(sc: SparkContext):
    """
    创建FileSystem
    :param sc SparkContext
    :return FileSystem对象
    """
    filesystem_class = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem
    hadoop_configuration = sc._jsc.hadoopConfiguration()
    return filesystem_class.get(hadoop_configuration)


def write_to_hdfs(sc: SparkContext, file_path, content, overwrite=True):
    """
    写内容到hdfs文件
    :param sc SparkContext
    :param file_path 绝对路径
    :param content 文件内容
    :param overwrite 是否覆盖
    """
    filesystem = _get_hdp_file_system(sc)
    out = filesystem.create(_get_hdp_path(sc, file_path), overwrite)
    out.write(bytearray(content, "utf-8"))
    out.flush()
    out.close()


def read_from_hdfs(sc: SparkContext, file_path):
    """
    从hdfs读取数据
    :param sc:
    :param file_path:
    :return:
    """
    text = "".join(sc.textFile(file_path).collect())
    return text


def get_file_list(path_pattern=[], root_path=""):
    """
    生成hdfs file list
    :param path_pattern:
    :param root_path
    :return:
    """
    cmd = """
        hadoop fs -ls -R {0}
    """.format(root_path)
    if len(path_pattern) == 0:
        pattern = "|".join(["(" + str(p.replace('/', '\/')) + ")" for p in path_pattern])
    else:
        pattern = ""

    def validate_path_pattern(path):
        if pattern != "" and re.search(pattern, path) and '_SUCCESS' not in path:
            return True
        elif pattern == "" and '_SUCCESS' not in path:
            return True
        else:
            return False

    status, output = subprocess.getstatusoutput(cmd)
    output = output.split('\n')
    output = list(filter(validate_path_pattern, output))
    file_list = list()
    polluted = any(len(info.split()) != 8 for info in output)
    if status == 0 and len(output) > 0 and not polluted:
        file_list = [info.split()[-1] for info in output if info[0] == '-']
    return file_list
